{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabb5f02-9dc8-4a54-b146-1f989f619472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import important libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import configparser\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# import mysql libraries\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "import mysql.connector\n",
    "\n",
    "# import important NLP tensorflow libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# import transformers libery to label the dataset\n",
    "from transformers import pipeline\n",
    "\n",
    "# convert emoji to text libery\n",
    "import emoji\n",
    "\n",
    "# tokenize libery\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# remove stopwords and punctuation libery\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Lemmatization libery\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# download corpus\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86df2d6-adfe-4c8c-93f8-2da70b77bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mysql_conf():\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read('config.ini')\n",
    "        username = config['mysql']['username']\n",
    "        password = config['mysql']['password']\n",
    "        hostname = config['mysql']['hostname']\n",
    "        database = config['mysql']['database']\n",
    "        return username,password,hostname,database\n",
    "\n",
    "def create_database():\n",
    "    username,password,hostname,database = mysql_conf()\n",
    "    conn = mysql.connector.connect(host = hostname,user = username,password = password)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(\"CREATE DATABASE IF NOT EXISTS twitter\")\n",
    "    cursor.close()\n",
    "\n",
    "\n",
    "def create_table():\n",
    "    username,password,hostname,database = mysql_conf()\n",
    "    conn = mysql.connector.connect(host = hostname,user = username,password = password, database = database)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(\"CREATE TABLE IF NOT EXISTS sentiment(tweet_id double NOT NULL,\\\n",
    "                                text text(65535) NOT NULL,\\\n",
    "                                tokenize text(65535) NOT NULL,\\\n",
    "                                sentiment varchar(25) NOT NULL,\\\n",
    "                                PRIMARY KEY (tweet_id ),\\\n",
    "                                FOREIGN KEY (tweet_id) REFERENCES datamined(tweet_id))\")\n",
    "    cursor.close()\n",
    "    \n",
    "def read_from_mysql():\n",
    "    username,password,hostname,database = mysql_conf()\n",
    "    engine = create_engine(\"mysql+pymysql://{user}:{pw}@{host}/{db}\".format(host= hostname, db= database, user= username, pw= password))\n",
    "    df = pd.read_sql('SELECT tweet_id,text FROM datamined where processed = False', con=engine)\n",
    "\n",
    "    return df\n",
    "\n",
    "def write_to_mysql(df):\n",
    "    username,password,hostname,database = mysql_conf()\n",
    "    df = df.drop(columns=['index'])\n",
    "    engine = create_engine(\"mysql+pymysql://{user}:{pw}@{host}/{db}\".format(host= hostname, db= database, user= username, pw= password))\n",
    "    df.to_sql('sentiment', con = engine, if_exists = 'append',index = False, chunksize = 1000)\n",
    "    \n",
    "    conn = mysql.connector.connect(host = hostname,user = username,password = password, database = database)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    quary = \"\"\"UPDATE datamined SET processed = %s WHERE tweet_id =  %s\"\"\"\n",
    "    for index in df.index:\n",
    "        val = (True,df[\"tweet_id\"][index])\n",
    "        cursor.execute(quary, val)\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    \n",
    "    print(\"................. Resetting Dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862410fb-70a1-4089-a60d-58ceee190881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning Dataframe\n",
    "def cleaning_df(preprocessed_df):\n",
    "    # remove hash sign for hashtag. preserve hashtag as it may contain information\n",
    "    preprocessed_df[\"text\"] = preprocessed_df[\"text\"].str.replace(\"#\", \"\", regex=True)\n",
    "\n",
    "    # remove website link\n",
    "    preprocessed_df[\"text\"] = preprocessed_df[\"text\"].str.replace(\"https?:\\\\/\\\\/[^\\\\s]+[\\\\r\\\\n]*\", \"\", regex=True)\n",
    "\n",
    "    # remove newline\n",
    "    preprocessed_df[\"text\"] = preprocessed_df[\"text\"].str.replace(\"\\n\", \" \", regex=True)\n",
    "\n",
    "    # Remove retweet, user tag\n",
    "    preprocessed_df[\"text\"] = preprocessed_df[\"text\"].str.replace('RT @[^\\s]+', \"\", regex=True)\n",
    "    preprocessed_df[\"text\"] = preprocessed_df[\"text\"].str.replace('@[^\\s]+', \"\", regex=True)\n",
    "\n",
    "    # remove $ symbol\n",
    "    preprocessed_df[\"text\"] = preprocessed_df[\"text\"].str.replace(\"$\", \"\", regex=True)\n",
    "\n",
    "    # remove ’ and '  symbol\n",
    "    preprocessed_df[\"text\"] = preprocessed_df[\"text\"].str.replace(\"’\", \"\", regex=True)\n",
    "    preprocessed_df[\"text\"] = preprocessed_df[\"text\"].str.replace(\"'\", \"\", regex=True)\n",
    "\n",
    "    # convert to lower case\n",
    "    preprocessed_df[\"text\"] = preprocessed_df[\"text\"].str.lower()\n",
    "    return  preprocessed_df\n",
    "\n",
    "#Convert emojis into text\n",
    "def convert_emoji(preprocessed_emoji_df):\n",
    "    # Convert emoji to text \n",
    "    preprocessed_emoji_df[\"text\"] = preprocessed_emoji_df[\"text\"].apply(emoji.demojize)\n",
    "    # remove Colon and double Colons \n",
    "    preprocessed_emoji_df[\"text\"] = preprocessed_emoji_df[\"text\"].str.replace(\"::\", \" \", regex=True)\n",
    "    preprocessed_emoji_df[\"text\"] = preprocessed_emoji_df[\"text\"].str.replace(\":\", \" \", regex=True)\n",
    "    preprocessed_emoji_df[\"text\"] = preprocessed_emoji_df[\"text\"].str.replace(\"_\", \" \", regex=True)\n",
    "    return preprocessed_emoji_df\n",
    "\n",
    "#Tokenization of the text\n",
    "def tokenize(preprocessed_token_df):\n",
    "    # tokenize\n",
    "    tokenizer = TweetTokenizer(preserve_case=False,\n",
    "                                strip_handles=True,\n",
    "                                reduce_len=True)\n",
    "    preprocessed_token_df['tokenize'] = preprocessed_token_df[\"text\"].apply(tokenizer.tokenize)\n",
    "    return preprocessed_token_df\n",
    "\n",
    "#Remove stopwords and punctuation\n",
    "def stopwords(preprocessed_stop_df):\n",
    "    from nltk.corpus import stopwords\n",
    "    # remove stopwords and punctuation\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    preprocessed_stop_df['tokenize'] =  preprocessed_stop_df['tokenize'].apply(lambda x: [item for item in x if item not in english_stopwords and item not in string.punctuation])\n",
    "    return preprocessed_stop_df\n",
    "\n",
    "#Lemmatization\n",
    "def lemmatization(preprocessed_lemmatization_df):\n",
    "    preprocessed_lemmatization_df['tokenize'] = preprocessed_lemmatization_df[\"tokenize\"].apply(lambda x: nltk.pos_tag(x))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    preprocessed_lemmatization_df['tokenize'] = preprocessed_lemmatization_df[\"tokenize\"].transform(lambda value: \" \".join([lemmatizer.lemmatize(a[0],pos=get_wordnet_pos(a[1])) if get_wordnet_pos(a[1]) else a[0] for a in value]))\n",
    "    # Sample code to see labels for first five rows in df\n",
    "    for i in preprocessed_lemmatization_df.index:\n",
    "      # now call function we defined above\n",
    "      if preprocessed_lemmatization_df[\"tokenize\"][i] == '' or None:\n",
    "            preprocessed_lemmatization_df = preprocessed_lemmatization_df.drop(i)\n",
    "        \n",
    "    # reset the datarame index \n",
    "    preprocessed_lemmatization_df = preprocessed_lemmatization_df.reset_index()\n",
    "    return preprocessed_lemmatization_df\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#sentiment Analysis with HuggingFace\n",
    "def huggingface(df,candidate_labels):\n",
    "    df[\"sentiment\"] = np.nan\n",
    "    # device=0 for GPU usage\n",
    "    classifier = pipeline(\"zero-shot-classification\",model=\"facebook/bart-large-mnli\", device=-1)\n",
    "\n",
    "    # Sample code to see labels for first five rows in df\n",
    "    for i in df.index:\n",
    "        # now call function we defined above\n",
    "        input_text = df['tokenize'][i]\n",
    "\n",
    "        # multi_label=True will return confidence score for both labels independently \n",
    "        model_dict = classifier(input_text, candidate_labels, multi_label=True)\n",
    "\n",
    "        # Zip results to dict\n",
    "        result_dict = dict(zip(model_dict.get('labels'), model_dict.get('scores')))\n",
    "\n",
    "        if ((result_dict.get('Positive') > result_dict.get('Negative')) and (result_dict.get('Positive') > result_dict.get('Neutral'))) :\n",
    "            sentiment = \"Positive\"\n",
    "        elif ((result_dict.get('Negative') > result_dict.get('Positive')) and (result_dict.get('Negative') > result_dict.get('Neutral'))) :\n",
    "            sentiment = \"Negative\"\n",
    "        else :\n",
    "            sentiment = \"Neutral\"\n",
    "        df['sentiment'][i] = sentiment\n",
    "        print(\"line number:\",i,\"out of\",len(df),\"lines\",round(i/len(df)*100,2),\"% completed\",end=\"\\r\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd76fce-0dc7-4d19-98bf-a894ceb940d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis():\n",
    "    # Output labels\n",
    "    candidate_labels = [\"Positive\",\"Negative\",\"Neutral\"]\n",
    "    \n",
    "    #Create Databases and tables\n",
    "    create_database()\n",
    "    create_table()\n",
    "    \n",
    "    while True:\n",
    "        df = read_from_mysql()\n",
    "        df = cleaning_df(df)\n",
    "        df = convert_emoji(df)\n",
    "        df = tokenize(df)\n",
    "        df = stopwords(df)\n",
    "        df = lemmatization(df)\n",
    "        df = huggingface(df,candidate_labels)\n",
    "        if df.empty == True:\n",
    "            print(\"\",end=\"\\r\")\n",
    "            print(\"........No new tweets, sleeping for 30 seconds!\")\n",
    "            time.sleep(30)\n",
    "            continue\n",
    "        write_to_mysql(df)\n",
    "        print(\"........sleeping for 5 seconds!\")\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7762caa-a278-44f3-84db-66b9e5e50411",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
